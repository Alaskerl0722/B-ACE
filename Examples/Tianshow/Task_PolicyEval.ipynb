{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programas\\b_ace_env\\Lib\\site-packages\\wandb\\analytics\\sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
      "  self.hub = sentry_sdk.Hub(client)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "import pandas as pd \n",
    "\n",
    "import random\n",
    "\n",
    "from torch.distributions import Normal, Distribution\n",
    "import pandas as pd\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import SubprocVectorEnv, DummyVectorEnv\n",
    "#from tianshou.env.pettingzoo_env_parallel import PettingZooParallelEnv\n",
    "#from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "#from PettingZooParallelEnv import PettingZooParal\n",
    "\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "\n",
    "from tianshou.utils.net.common import ActorCritic, DataParallelNet, Net\n",
    "from tianshou.utils.net.continuous import Actor, Critic\n",
    "\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, DDPGPolicy\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from DNN_B_ACE_ACTOR import DNN_B_ACE_ACTOR\n",
    "from DNN_B_ACE_CRITIC import DNN_B_ACE_CRITIC\n",
    "from Task_MHA_B_ACE import Task_MHA_B_ACE\n",
    "from Task_DNN_B_ACE import Task_DNN_B_ACE\n",
    "from Task_B_ACE_Env import B_ACE_TaskEnv\n",
    "\n",
    "from CollectorMA import CollectorMA\n",
    "from MAParalellPolicy import MAParalellPolicy\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "\n",
    "####---------------------------#######\n",
    "#Tianshou Adjustment\n",
    "import wandb\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Tianshow_Training_GoDot.ipybn\"\n",
    "from tianshou.utils import WandbLogger\n",
    "# from tianshou.utils.logger.base import LOG_DATA_TYPE\n",
    "# def new_write(self, step_type: str, step: int, data: LOG_DATA_TYPE) -> None:\n",
    "#      data[step_type] = step\n",
    "#      wandb.log(data)   \n",
    "# WandbLogger.write = new_write \n",
    "####---------------------------#######\n",
    "\n",
    "\n",
    "model  =  \"Task_MHA_B_ACE\"#\"SISL_Task_MultiHead\" #\"CNN_ATT_SISL\" #\"MultiHead_SISL\" \n",
    "test_num  =  \"_B_ACE03\"\n",
    "policyModel  =  \"DQN\"\n",
    "name = model + test_num\n",
    "\n",
    "train_env_num = 4\n",
    "test_env_num  = 15\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn_sisl\", log_name)\n",
    "\n",
    "#Duck Training Best\n",
    "#load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE02240721-151049_1261_BestRew.pth'\n",
    "\n",
    "#FSMG1 Training Best\n",
    "load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241019-215530_1893_BestRew.pth'\n",
    "\n",
    "\n",
    "\n",
    "save_policy_name = f'policy_{log_name}'\n",
    "policy_path = model + policyModel\n",
    "\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "Policy_Config = {\n",
    "    \"same_policy\" : True,\n",
    "    \"load_model\" : False,\n",
    "    \"freeze_CNN\" : False     \n",
    "                }\n",
    "\n",
    "B_ACE_Config = { \t\n",
    "                    \"EnvConfig\" : \n",
    "                    {\n",
    "                        \"task\": \"b_ace_v1\",\n",
    "                        \"env_path\": \"../../BVR_AirCombat/bin/B_ACE_v13.exe\",\n",
    "                        \"port\": 12500,\n",
    "                        \"renderize\": 0,\n",
    "                        \"debug_view\": 0,\n",
    "                        \"phy_fps\": 20,\n",
    "                        \"speed_up\": 50000,\n",
    "                        \"max_cycles\": 36000,\n",
    "                        \"experiment_mode\"  : 0,\n",
    "                        \"parallel_envs\": 1,\t\n",
    "                        \"seed\": 1,\t\n",
    "                        \"action_repeat\": 20,\t\n",
    "                        \"action_type\": \"Low_Level_Continuous\",                        \n",
    "                        \"stop_mission\" : 1,\n",
    "                        \n",
    "                        \n",
    "                        \"RewardsConfig\" : {\n",
    "                                    \"mission_factor\": 0.001,\t\t\t\t\n",
    "                                    \"missile_fire_factor\": -0.1,\t\t\n",
    "                                    \"missile_no_fire_factor\": -0.001,\n",
    "                                    \"missile_miss_factor\": -0.5,\n",
    "                                    \"detect_loss_factor\": -0.1,\n",
    "                                    \"keep_track_factor\": 0.001,\n",
    "                                    \"hit_enemy_factor\": 3.0,\n",
    "                                    \"hit_own_factor\": -5.0,\t\t\t\n",
    "                                    \"mission_accomplished_factor\": 10.0,\t\t\t\n",
    "                                }\n",
    "                    },\n",
    "\n",
    "                    \"AgentsConfig\" : \n",
    "                    {\n",
    "                        \"blue_agents\": { \n",
    "                            \"num_agents\" : 1,\n",
    "                            \"mission\"    : \"DCA\",\n",
    "                            \"beh_config\" : {\n",
    "                                            \"dShot\" : [1.04, 0.50, 1.09],\n",
    "                                            \"lCrank\": [1.06, 0.98, 0.98],\n",
    "                                            \"lBreak\": [1.05, 1.17, 0.45]\n",
    "                                        },\n",
    "                            \"base_behavior\": \"external\",                  \n",
    "                            \"init_position\": {\"x\": 0.0, \"y\": 25000.0,\"z\": 30.0},\n",
    "                            \"offset_pos\": {\t\"x\": 0.0, \"y\": 0.0, \"z\": 0.0},\n",
    "                            \"init_hdg\": 0.0,                        \n",
    "                            \"target_position\": {\"x\": 0.0,\"y\": 25000.0,\"z\": 30.0},\n",
    "                            \"rnd_offset_range\":{\"x\": 10.0,\"y\": 10000.0,\"z\": 5.0},\t\t\t\t\n",
    "                            \"rnd_shot_dist_var\": 0.025,\n",
    "                            \"rnd_crank_var\": 0.025,\n",
    "                            \"rnd_break_var\": 0.025,\n",
    "                            \"wez_models\" : \"res://assets/wez/Default_Wez_params.json\"\n",
    "                        },\t\n",
    "                        \"red_agents\":\n",
    "                        { \n",
    "                            \"num_agents\" : 1, \n",
    "                            \"base_behavior\": \"duck\",\n",
    "                            \"mission\"    : \"striker\",\n",
    "                            \"beh_config\" : {\n",
    "                                              \"dShot\" : [1.04, 1.04, 1.04], #[1.04, 0.50, 1.09]\n",
    "                                              \"lCrank\": [1.06, 1.06, 1.06], #1.06, 0.98, 0.98\n",
    "                                              \"lBreak\": [1.05, 1.05, 1.05] #1.05, 1.17, 0.45\n",
    "                                          },\n",
    "                            # \"beh_config\" : {\n",
    "                            #                \"dShot\" : [1.04, 0.50, 1.09],\n",
    "                            #                 \"lCrank\": [1.06, 0.98, 0.98],\n",
    "                            #                 \"lBreak\": [1.05, 1.17, 0.45]\n",
    "                            #             },\n",
    "                            \"init_position\": {\"x\": 0.0,\"y\": 25000.0,\"z\": -30.0},\n",
    "                            \"offset_pos\": {\"x\": 0.0,\"y\": 0.0,\"z\": 0.0},\n",
    "                            \"init_hdg\" : 180.0,                        \n",
    "                            \"target_position\": {\"x\": 0.0,\"y\": 25000.0,\"z\": 30.0},\n",
    "                            \"rnd_offset_range\":{\"x\": 10.0,\"y\": 10000.0,\"z\": 5.0},\t\t\t\t\n",
    "                            \"rnd_shot_dist_var\": 0.025,\n",
    "                            \"rnd_crank_var\": 0.025,\n",
    "                            \"rnd_break_var\": 0.025,\n",
    "                            \"wez_models\" : \"res://assets/wez/Default_Wez_params.json\"\n",
    "                        }\n",
    "                    }\t\n",
    "            }\n",
    "#max_cycles = B_ACE_Config[\"max_cycles\"]\n",
    "#n_agents = 1#B_ACE_Config[\"n_pursuers\"]\n",
    "\n",
    "dqn_params =    {\n",
    "                \"discount_factor\": 0.99, \n",
    "                \"estimation_step\": 180, \n",
    "                \"target_update_freq\": 6000 * 3 ,#max_cycles * n_agents,\n",
    "                \"reward_normalization\" : False,\n",
    "                \"clip_loss_grad\" : False,\n",
    "                \"optminizer\": \"Adam\",\n",
    "                \"lr\": 0.00005, \n",
    "                \"max_tasks\" : 30\n",
    "                }\n",
    "\n",
    "PPO_params= {    \n",
    "                'action_scaling': True,\n",
    "                'discount_factor': 0.98,\n",
    "                'max_grad_norm': 0.5,\n",
    "                'eps_clip': 0.2,\n",
    "                'vf_coef': 0.5,\n",
    "                'ent_coef': 0.01,\n",
    "                'gae_lambda': 0.95,\n",
    "                'reward_normalization': False, \n",
    "                'dual_clip': None,\n",
    "                'value_clip': False,   \n",
    "                'deterministic_eval': True,\n",
    "                'advantage_normalization': False,\n",
    "                'recompute_advantage': False,\n",
    "                'action_bound_method': \"clip\",\n",
    "                'lr_scheduler': None,\n",
    "            }\n",
    "\n",
    "\n",
    "trainer_params = {\"max_epoch\": 500,\n",
    "                  \"step_per_epoch\": 18000,#5 * (150 * n_agents),\n",
    "                  \"step_per_collect\": 6000,# * (10 * n_agents),\n",
    "                  \n",
    "                  \"batch_size\" : 1024,\n",
    "                  \n",
    "                  \"update_per_step\": 1 / (100), #Off-Policy Only (run after close a Collect (run many times as necessary to meet the value))\n",
    "                  \n",
    "                  \"repeat_per_collect\": 32, #On-Policy Only\n",
    "                  \n",
    "                  \"episode_per_test\": 30,                  \n",
    "                  \"tn_eps_max\": 0.20,\n",
    "                  \"ts_eps_max\": 0.01,\n",
    "                  \"warmup_size\" : 1,\n",
    "                  \"train_envs\" : train_env_num,\n",
    "                  \"test_envs\" : test_env_num\n",
    "}\n",
    "#agent_learn = PPOPolicy(**policy_params)\n",
    "\n",
    "\n",
    "runConfig = dqn_params\n",
    "runConfig.update(Policy_Config)\n",
    "runConfig.update(B_ACE_Config)\n",
    "runConfig.update(trainer_params) \n",
    "runConfig.update(dqn_params)\n",
    "\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()       \n",
    "    agent_observation_space = env.observation_space(\"agent_0\")\n",
    "   \n",
    "    #print(env.action_space)\n",
    "    #action_shape = 50#env.action_space.shape\n",
    "    \n",
    "    #print(\"ActionSPACE: \", env.action_space)\n",
    "    action_space = env.action_space\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"  \n",
    "\n",
    "    agents = []        \n",
    "    \n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "        policies_number = 1\n",
    "    else:\n",
    "        policies_number = len(env.agents)\n",
    "\n",
    "    for _ in range(policies_number):      \n",
    "        \n",
    "        #print(agent_observation_space)\n",
    "        \n",
    "        if policyModel == \"DQN\":\n",
    "\n",
    "            if model == \"Task_MHA_B_ACE\":\n",
    "                net = Task_MHA_B_ACE(\n",
    "                    #obs_shape=agent_observation_space.shape,                                                  \n",
    "                    num_tasks = dqn_params[\"max_tasks\"],\n",
    "                    num_features_per_task= 14,                    \n",
    "                    nhead = 4,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                    \n",
    "                ).to(device) \n",
    "            \n",
    "            if model == \"Task_DNN_B_ACE\":\n",
    "                net = Task_DNN_B_ACE(\n",
    "                    #obs_shape=agent_observation_space.shape,                                                  \n",
    "                    num_tasks = dqn_params[\"max_tasks\"],\n",
    "                    num_features_per_task= 14,                    \n",
    "                    nhead = 4,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                    \n",
    "                ).to(device) \n",
    "                \n",
    "            optim = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True)       \n",
    "            \n",
    "            agent_learn = DQNPolicy(\n",
    "                model=net,\n",
    "                optim=optim,\n",
    "                action_space = Discrete(dqn_params[\"max_tasks\"]),\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "                reward_normalization = dqn_params[\"reward_normalization\"],\n",
    "                clip_loss_grad = dqn_params[\"clip_loss_grad\"]\n",
    "            )                   \n",
    "        \n",
    "        elif model == \"PPO_DNN\":\n",
    "            \n",
    "            actor = DNN_B_ACE_ACTOR(\n",
    "                obs_shape=agent_observation_space.shape[0],                \n",
    "                action_shape=4,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"                \n",
    "            ).to(device)\n",
    "\n",
    "            critic = DNN_B_ACE_CRITIC(\n",
    "                obs_shape=agent_observation_space.shape[0],                \n",
    "                action_shape=4,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"                \n",
    "            ).to(device)\n",
    "            \n",
    "                                    \n",
    "            actor_critic = ActorCritic(actor, critic)\n",
    "        \n",
    "            # orthogonal initialization\n",
    "            # for m in actor_critic.modules():\n",
    "            #     if isinstance(m, torch.nn.Linear):\n",
    "            #         torch.nn.init.orthogonal_(m.weight)\n",
    "            #         torch.nn.init.zeros_(m.bias)            \n",
    "            \n",
    "            # dist = torch.distributions.Normal(torch.tensor([0.0]), torch.tensor([1.0])) \n",
    "                # define policy\n",
    "            def dist(mu, sigma) -> Distribution:\n",
    "                return Normal(mu, sigma)        \n",
    "                \n",
    "            #optim_actor  = torch.optim.Adam(netActor.parameters(),  lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "            #optim_critic = torch.optim.Adam(netCritic.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "            optim = torch.optim.Adam(actor_critic.parameters(), lr=dqn_params[\"lr\"])\n",
    "                    \n",
    "            agent_learn = PPOPolicy(\n",
    "                actor=actor,\n",
    "                critic=critic,\n",
    "                optim=optim,\n",
    "                dist_fn=dist,                \n",
    "                action_scaling  =       PPO_params['action_scaling'],\n",
    "                discount_factor =       PPO_params['discount_factor'],\n",
    "                max_grad_norm   =       PPO_params['max_grad_norm'],\n",
    "                eps_clip        =       PPO_params['eps_clip'],\n",
    "                vf_coef         =       PPO_params['vf_coef'],\n",
    "                ent_coef        =       PPO_params['ent_coef'],\n",
    "                gae_lambda      =       PPO_params['gae_lambda'],\n",
    "                reward_normalization=   PPO_params['reward_normalization'],\n",
    "                action_space    =  action_space,\n",
    "                deterministic_eval=     PPO_params['deterministic_eval'],\n",
    "                advantage_normalization=PPO_params['advantage_normalization'],\n",
    "                recompute_advantage=    PPO_params['recompute_advantage'],\n",
    "                action_bound_method=    PPO_params['action_bound_method'],\n",
    "                lr_scheduler=None\n",
    "            )\n",
    "            \n",
    "        if Policy_Config[\"load_model\"] is True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "                   \n",
    "        \n",
    "        agents.append(agent_learn)\n",
    "\n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "        agents = [agents[0] for _ in range(len(env.agents))]\n",
    "    else:\n",
    "        for _ in range(len(env.agents) - policies_number):\n",
    "            agents.append(agents[0])\n",
    "    \n",
    "    policy = MultiAgentPolicyManager(policies = agents, env=env)  \n",
    "    #policy = MAParalellPolicy(policies = agents, env=env, device=\"cuda\" if torch.cuda.is_available() else \"cpu\" )  \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    \n",
    "    B_ACE_Config[\"EnvConfig\"][\"seed\"] = random.randint(0, 1000000)\n",
    "    \n",
    "    env = B_ACE_TaskEnv( convert_action_space = True,\n",
    "                                    device = \"cpu\",\n",
    "                                    **B_ACE_Config)\n",
    "    \n",
    "    #env.action_space = env.action_space()\n",
    "    #env = PettingZooEnv(env)  \n",
    "    \n",
    "    return env \n",
    "\n",
    "# Function to update the red agent's behavior in the B_ACE_Config dictionary\n",
    "def update_red_agent_behavior(config, blue_agents_num, red_agents_num, base_behavior, mission, dShot, lCrank, lBreak):\n",
    "    config[\"AgentsConfig\"][\"blue_agents\"][\"num_agents\"] = blue_agents_num\n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"num_agents\"] = red_agents_num\n",
    "    \n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"base_behavior\"] = base_behavior\n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"mission\"] = mission\n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"beh_config\"][\"dShot\"] = dShot\n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"beh_config\"][\"lCrank\"] = lCrank\n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"beh_config\"][\"lBreak\"] = lBreak\n",
    "    \n",
    "\n",
    "# Function to compute mean and 95% bootstrap confidence interval\n",
    "def compute_mean_and_ci(data, confidence_level=0.95):\n",
    "    mean_value = np.mean(data)\n",
    "    # Perform bootstrap resampling to compute confidence interval\n",
    "    res = bootstrap((data,), np.mean, confidence_level=confidence_level, n_resamples=10000, method='basic')\n",
    "    ci_lower, ci_upper = res.confidence_interval\n",
    "    return mean_value, ci_lower, ci_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for remote GODOT connection on port 13253\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [41], 'space': 'box'}}\n",
      "waiting for remote GODOT connection on port 13526\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [41], 'space': 'box'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_142972\\665638509.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy.policies[agent].load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  32.47866542078507\n",
      "Std:   0.32068715124778446\n",
      "Max:   32.935248997176394\n",
      "Min:   31.54432512674727\n",
      "  eval_enemy training_beh  team   metric       mean   ci_lower   ci_upper\n",
      "0     FSM_B1       FSM_B1  Blue   killed   0.000000   0.000000   0.000000\n",
      "1     FSM_B1       FSM_B1   Red   killed   2.000000   2.000000   2.000000\n",
      "2     FSM_B1       FSM_B1  Blue  missile   2.066667   1.966667   2.133333\n",
      "3     FSM_B1       FSM_B1   Red  missile   0.000000   0.000000   0.000000\n",
      "4     FSM_B1       FSM_B1  Blue  mission   0.000000   0.000000   0.000000\n",
      "5     FSM_B1       FSM_B1   Red  mission   0.000000   0.000000   0.000000\n",
      "6     FSM_B1       FSM_B1  Blue   reward  32.478665  32.370740  32.602621\n",
      "7     FSM_B1       FSM_B1   Red   reward   0.000000   0.000000   0.000000\n",
      "close message sent\n",
      "waiting for remote GODOT connection on port 12287\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [41], 'space': 'box'}}\n",
      "waiting for remote GODOT connection on port 13693\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [41], 'space': 'box'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_142972\\665638509.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy.policies[agent].load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  31.371146884316897\n",
      "Std:   0.839263863835147\n",
      "Max:   32.8830249407978\n",
      "Min:   29.27956559968197\n",
      "  eval_enemy training_beh  team   metric       mean   ci_lower   ci_upper\n",
      "0     FSM_B1       FSM_G3  Blue   killed   0.000000   0.000000   0.000000\n",
      "1     FSM_B1       FSM_G3   Red   killed   2.000000   2.000000   2.000000\n",
      "2     FSM_B1       FSM_G3  Blue  missile   3.333333   3.066667   3.600000\n",
      "3     FSM_B1       FSM_G3   Red  missile   0.000000   0.000000   0.000000\n",
      "4     FSM_B1       FSM_G3  Blue  mission   0.000000   0.000000   0.000000\n",
      "5     FSM_B1       FSM_G3   Red  mission   0.000000   0.000000   0.000000\n",
      "6     FSM_B1       FSM_G3  Blue   reward  31.371147  31.072980  31.677668\n",
      "7     FSM_B1       FSM_G3   Red   reward   0.000000   0.000000   0.000000\n",
      "close message sent\n",
      "waiting for remote GODOT connection on port 13937\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [41], 'space': 'box'}}\n",
      "waiting for remote GODOT connection on port 11266\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [41], 'space': 'box'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_142972\\665638509.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy.policies[agent].load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  10.265479814788554\n",
      "Std:   20.014542893414205\n",
      "Max:   32.94473585520268\n",
      "Min:   -26.145978475481847\n",
      "  eval_enemy training_beh  team   metric       mean   ci_lower   ci_upper\n",
      "0     FSM_G3       FSM_B1  Blue   killed   0.000000   0.000000   0.000000\n",
      "1     FSM_G3       FSM_B1   Red   killed   1.823529   1.647059   2.000000\n",
      "2     FSM_G3       FSM_B1  Blue  missile   3.058824   2.235294   3.705882\n",
      "3     FSM_G3       FSM_B1   Red  missile   0.117647  -0.058824   0.235294\n",
      "4     FSM_G3       FSM_B1  Blue  mission   0.000000   0.000000   0.000000\n",
      "5     FSM_G3       FSM_B1   Red  mission   0.176471   0.000000   0.352941\n",
      "6     FSM_G3       FSM_B1  Blue   reward  21.051343  11.538524  31.107161\n",
      "7     FSM_G3       FSM_B1   Red   reward   0.000000   0.000000   0.000000\n",
      "close message sent\n",
      "waiting for remote GODOT connection on port 11677\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [41], 'space': 'box'}}\n",
      "waiting for remote GODOT connection on port 13352\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [41], 'space': 'box'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_142972\\665638509.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy.policies[agent].load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  18.998745893608778\n",
      "Std:   15.02178144352105\n",
      "Max:   31.887658285650748\n",
      "Min:   -6.072678667755855\n",
      "  eval_enemy training_beh  team   metric       mean   ci_lower   ci_upper\n",
      "0     FSM_G3       FSM_G3  Blue   killed   0.000000   0.000000   0.000000\n",
      "1     FSM_G3       FSM_G3   Red   killed   1.909091   1.818182   2.045455\n",
      "2     FSM_G3       FSM_G3  Blue  missile   4.318182   3.590909   5.000000\n",
      "3     FSM_G3       FSM_G3   Red  missile   0.090909  -0.045455   0.181818\n",
      "4     FSM_G3       FSM_G3  Blue  mission   0.000000   0.000000   0.000000\n",
      "5     FSM_G3       FSM_G3   Red  mission   0.000000   0.000000   0.000000\n",
      "6     FSM_G3       FSM_G3  Blue   reward  27.659283  25.682322  30.032230\n",
      "7     FSM_G3       FSM_G3   Red   reward   0.000000   0.000000   0.000000\n",
      "close message sent\n"
     ]
    }
   ],
   "source": [
    "test_env_num = 1       \n",
    "seed = 1\n",
    "episodes =  30\n",
    "render  = False\n",
    "\n",
    "#Test Conditions\n",
    "Blue_test_agents = 2\n",
    "Red_test_agents = 2\n",
    "\n",
    "Enemies = [\"Duck\", \"FSM_B1\",\"FSM_G3\", \"FSM_G10\"]\n",
    "Enemies = [\"FSM_B1\", \"FSM_G3\"]\n",
    "\n",
    "\n",
    "#Training Model selection\n",
    "Blue_training_agents = 2\n",
    "Red_training_agents = 2\n",
    "\n",
    "Training_Bases = [\"Duck\", \"FSM_B1\",\"FSM_G3\", \"FSM_G10\"]\n",
    "Training_Bases = [\"FSM_B1\", \"FSM_G3\"]\n",
    "\n",
    "\n",
    "# \"policy_Task_MHA_B_ACE_B_ACE03241024-165541_1185_BestRew\" / 1 x 2 FSMG10\n",
    "\n",
    "for Enemy_type in Enemies:\n",
    "    for Training_Enemy in Training_Bases:\n",
    "                        \n",
    "        if Blue_training_agents == 1 and Red_training_agents == 1:\n",
    "            \n",
    "            if Training_Enemy == \"Duck\":    \n",
    "                load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241018-082438_1669_BestRew.pth'\n",
    "                TCode = 11\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_B1\":\n",
    "                load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241017-210033_1301_BestRew.pth'\n",
    "                TCode = 21\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G3\":            \n",
    "                load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241019-215530_1893_BestRew.pth'\n",
    "                TCode = 31\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G10\":            \n",
    "                load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241022-144023_1329_BestRew.pth'\n",
    "                TCode = 41\n",
    "        \n",
    "        elif Blue_training_agents == 1 and Red_training_agents == 2:\n",
    "            \n",
    "            if Training_Enemy == \"Duck\":    \n",
    "                #load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241018-082438_1669_BestRew.pth'\n",
    "                TCode = 12\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_B1\":\n",
    "                #load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241017-210033_1301_BestRew.pth'\n",
    "                TCode = 22\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G3\":            \n",
    "                #load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241019-215530_1893_BestRew.pth'\n",
    "                TCode = 32\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G10\":            \n",
    "                load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241024-165541_1185_BestRew.pth'\n",
    "                TCode = 42\n",
    "        \n",
    "        elif Blue_training_agents == 2 and Red_training_agents == 2:\n",
    "            \n",
    "            if Training_Enemy == \"Duck\":    \n",
    "                #load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241018-082438_1669_BestRew.pth'\n",
    "                TCode = 13\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_B1\":\n",
    "                load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241118-103140_573_BestRew.pth'\n",
    "                TCode = 23\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G3\":            \n",
    "                load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241112-084823_1689_BestRew.pth'\n",
    "                TCode = 33\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G10\":            \n",
    "                #load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241024-165541_1185_BestRew.pth'\n",
    "                TCode = 43\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Update B-ACE Enemy Config\n",
    "        if Enemy_type == \"Duck\":\n",
    "            new_enemy_behavior = \"duck\"   \n",
    "            new_mission = \"striker\"\n",
    "            new_dShot  = [1.04]\n",
    "            new_lCrank = [1.06]\n",
    "            new_lBreak = [1.05]\n",
    "             \n",
    "            ECode = 1\n",
    "            \n",
    "        elif Enemy_type == \"FSM_B1\":\n",
    "            new_enemy_behavior = \"baseline1\"\n",
    "            new_mission = \"striker\"\n",
    "            new_dShot  = [1.04]\n",
    "            new_lCrank = [1.06]\n",
    "            new_lBreak = [1.05]\n",
    "            ECode = 2\n",
    "            \n",
    "        elif Enemy_type == \"FSM_G3\":\n",
    "            new_enemy_behavior = \"baseline1\"\n",
    "            new_mission = \"striker\"\n",
    "           # Data for 3 agents\n",
    "            new_dShot = [0.50, 0.99, 1.04]\n",
    "            new_lCrank = [0.98, 0.96, 1.14]\n",
    "            new_lBreak = [1.17, 0.51, 1.05]                      \n",
    "\n",
    "            ECode = 3\n",
    "        \n",
    "        elif Enemy_type == \"FSM_G10\":\n",
    "            new_enemy_behavior = \"baseline1\"\n",
    "            new_mission = \"striker\"\n",
    "            # Data for 10 agents\n",
    "            new_dShot = [0.50, 0.99, 1.04, 0.50, 0.99, 0.93, 0.57, 0.50, 0.50, 0.50]\n",
    "            new_lCrank = [0.64, 0.96, 1.14, 0.69, 0.96, 0.69, 1.07, 0.20, 0.98, 0.69]\n",
    "            new_lBreak = [1.17, 0.51, 1.05, 0.25, 0.84, 0.51, 0.61, 0.37, 1.17, 0.51]            \n",
    "            ECode = 4\n",
    "\n",
    "\n",
    "        # Update the red agent's behavior\n",
    "        update_red_agent_behavior(\n",
    "            B_ACE_Config,\n",
    "            Blue_test_agents,\n",
    "            Red_test_agents, \n",
    "            new_enemy_behavior, \n",
    "            new_mission, \n",
    "            new_dShot, \n",
    "            new_lCrank, \n",
    "            new_lBreak\n",
    "        )\n",
    "        \n",
    "        policy, optim, agents = _get_agents()        \n",
    "        test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)])         \n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "                \n",
    "        \n",
    "        # Load the saved checkpoint\n",
    "        for agent in agents:    \n",
    "            \n",
    "            if Policy_Config[\"same_policy\"]:\n",
    "                model_path = os.path.join(\"Task_MHA_B_ACEDQN\", load_policy_name)                            \n",
    "            else:\n",
    "                model_path = os.path.join(\"Task_MHA_B_ACEDQN\", load_policy_name) \n",
    "\n",
    "            policy.policies[agent].set_eps(0.00)\n",
    "            policy.policies[agent].load_state_dict(torch.load(model_path))\n",
    "            policy.policies[agent].eval()\n",
    "            \n",
    "        test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "\n",
    "        results = test_collector.collect(n_episode=episodes)#0.02)#, gym_reset_kwargs={'seed' :2})\n",
    "\n",
    "        print(\"Mean: \", np.mean(results.returns))\n",
    "        print(\"Std:  \" , np.std (results.returns))\n",
    "        print(\"Max:  \" , np.max( results.returns))\n",
    "        print(\"Min:  \" , np.min(results.returns))\n",
    "\n",
    "        #Gets Final Stats\n",
    "        methods = test_envs.get_env_attr(\"call_results\")  # This returns a list of method references\n",
    "        results = [method() for method in methods]  # Call each method\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        data = results\n",
    "\n",
    "        blue_team_data = []\n",
    "        red_team_data = []\n",
    "        general_data = []\n",
    "\n",
    "        for episode in data:\n",
    "            for entry in episode:\n",
    "                blue_team_data.append(entry[0])  # First dictionary: Blue team\n",
    "                red_team_data.append(entry[1])   # Second dictionary: Red team\n",
    "                general_data.append(entry[2])    # Third dictionary: General sim data\n",
    "\n",
    "        # Convert to DataFrames\n",
    "        df_blue = pd.DataFrame(blue_team_data)\n",
    "        df_blue = df_blue[df_blue.end_cond.notna()]\n",
    "        \n",
    "        df_blue['training_beh'] = Training_Enemy\n",
    "        df_blue['Blue_test_agents_num'] = Blue_test_agents\n",
    "        df_blue['Blue_training_agents_num'] = Blue_training_agents                \n",
    "\n",
    "        df_red = pd.DataFrame(red_team_data)\n",
    "        df_red = df_red[df_red.end_cond.notna()]  \n",
    "        df_red['eval_enemy'] = Enemy_type      \n",
    "        df_blue['Red_test_agents_num'] = Red_test_agents\n",
    "        df_blue['Red_training_agents_num'] = Red_training_agents            \n",
    "\n",
    "        df_general = pd.DataFrame(general_data)\n",
    "\n",
    "        # Merge DataFrames for a complete view (optional)\n",
    "        df_merged = pd.concat([df_general, df_blue.add_prefix('blue_'), df_red.add_prefix('red_')], axis=1)\n",
    "        df_merged = df_merged[df_merged.blue_end_cond.notna()]\n",
    "\n",
    "        # Compute mean and confidence intervals for Blue and Red team metrics\n",
    "        metrics = ['killed', 'missile', 'mission', 'reward']\n",
    "\n",
    "        final_results = {'eval_enemy' : [], 'training_beh' : [], 'team': [], 'metric': [], 'mean': [], 'ci_lower': [], 'ci_upper': []}\n",
    "\n",
    "        for metric in metrics:\n",
    "            for team, df in [('Blue', df_blue), ('Red', df_red)]:\n",
    "                mean_value, ci_lower, ci_upper = compute_mean_and_ci(df[metric])\n",
    "                final_results['eval_enemy'].append(Enemy_type)\n",
    "                final_results['training_beh'].append(Training_Enemy)\n",
    "                final_results['team'].append(team)\n",
    "                final_results['metric'].append(metric)\n",
    "                final_results['mean'].append(mean_value)\n",
    "                final_results['ci_lower'].append(ci_lower)\n",
    "                final_results['ci_upper'].append(ci_upper)\n",
    "\n",
    "        # Convert the results into a DataFrame for display\n",
    "        df_final_results = pd.DataFrame(final_results)\n",
    "        print(df_final_results)\n",
    "\n",
    "        df_merged.to_csv(f'{Blue_test_agents}x{Red_test_agents}_{TCode}.{ECode}.Merged_Eval_{Training_Enemy}_Training_{Enemy_type}_Enemy.csv')\n",
    "        df_final_results.to_csv(f'{TCode}.{ECode}.Final_Eval_{Training_Enemy}_Training_{Enemy_type}_Enemy.csv')\n",
    "        \n",
    "        # Clean up and close the environments\n",
    "        test_collector.reset()\n",
    "        test_envs.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b_ace_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
