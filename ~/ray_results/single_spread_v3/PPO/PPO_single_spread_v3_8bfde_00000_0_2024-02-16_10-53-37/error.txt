Failure # 1 (occurred at 2024-02-16_10-53-46)
The actor died because of an error raised in its creation task, [36mray::PPO.__init__()[39m (pid=111147, ip=172.23.148.251, actor_id=8eb16805caaf7d7159be104d01000000, repr=PPO)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py", line 229, in _setup
    self.add_workers(
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py", line 616, in add_workers
    raise result.get()
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py", line 487, in __fetch_result
    result = ray.get(r)
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=111256, ip=172.23.148.251, actor_id=012ca2c0fd7968298492667e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f85115c10f0>)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py", line 206, in step
    obss, rews, terminateds, truncateds, infos = self.par_env.step(action_dict)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/conversions.py", line 207, in step
    self.aec_env.step(actions[agent])
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/order_enforcing.py", line 91, in step
    super().step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py", line 116, in step
    self.env.step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/clip_out_of_bounds.py", line 52, in step
    super().step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py", line 116, in step
    self.env.step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py", line 253, in step
    self._execute_world_step()
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py", line 173, in _execute_world_step
    scenario_action.append(action[0:mdim])
TypeError: 'int' object is not subscriptable

The above exception was the direct cause of the following exception:

[36mray::RolloutWorker.__init__()[39m (pid=111256, ip=172.23.148.251, actor_id=012ca2c0fd7968298492667e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f85115c10f0>)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py", line 81, in check_env
    check_multiagent_environments(env)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py", line 368, in check_multiagent_environments
    raise ValueError(
ValueError: Your environment (<ParallelPettingZooEnv instance>) does not abide to the new gymnasium-style API!
From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
In particular, the `step()` method seems to be faulty.
Learn more about the most important changes here:
https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium

In order to fix this problem, do the following:

1) Run `pip install gymnasium` on your command line.
2) Change all your import statements in your code from
   `import gym` -> `import gymnasium as gym` OR
   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`

For your custom (single agent) gym.Env classes:
3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import
     EnvCompatibility` wrapper class.
3.2) Alternatively to 3.1:
 - Change your `reset()` method to have the call signature 'def reset(self, *,
   seed=None, options=None)'
 - Return an additional info dict (empty dict should be fine) from your `reset()`
   method.
 - Return an additional `truncated` flag from your `step()` method (between `done` and
   `info`). This flag should indicate, whether the episode was terminated prematurely
   due to some time constraint or other kind of horizon setting.

For your custom RLlib `MultiAgentEnv` classes:
4.1) Either wrap your old MultiAgentEnv via the provided
     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import
     MultiAgentEnvCompatibility` wrapper class.
4.2) Alternatively to 4.1:
 - Change your `reset()` method to have the call signature
   'def reset(self, *, seed=None, options=None)'
 - Return an additional per-agent info dict (empty dict should be fine) from your
   `reset()` method.
 - Rename `dones` into `terminateds` and only set this to True, if the episode is really
   done (as opposed to has been terminated prematurely due to some horizon/time-limit
   setting).
 - Return an additional `truncateds` per-agent dictionary flag from your `step()`
   method, including the `__all__` key (100% analogous to your `dones/terminateds`
   per-agent dict).
   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This
   flag should indicate, whether the episode (for some agent or all agents) was
   terminated prematurely due to some time constraint or other kind of horizon setting.


During handling of the above exception, another exception occurred:

[36mray::RolloutWorker.__init__()[39m (pid=111256, ip=172.23.148.251, actor_id=012ca2c0fd7968298492667e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f85115c10f0>)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py", line 414, in __init__
    check_env(self.env, self.config)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py", line 96, in check_env
    raise ValueError(
ValueError: Traceback (most recent call last):
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py", line 363, in check_multiagent_environments
    results = env.step(sampled_action)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py", line 206, in step
    obss, rews, terminateds, truncateds, infos = self.par_env.step(action_dict)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/conversions.py", line 207, in step
    self.aec_env.step(actions[agent])
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/order_enforcing.py", line 91, in step
    super().step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py", line 116, in step
    self.env.step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/clip_out_of_bounds.py", line 52, in step
    super().step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py", line 116, in step
    self.env.step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py", line 253, in step
    self._execute_world_step()
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py", line 173, in _execute_world_step
    scenario_action.append(action[0:mdim])
TypeError: 'int' object is not subscriptable

The above exception was the direct cause of the following exception:

[36mray::RolloutWorker.__init__()[39m (pid=111256, ip=172.23.148.251, actor_id=012ca2c0fd7968298492667e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f85115c10f0>)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py", line 81, in check_env
    check_multiagent_environments(env)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py", line 368, in check_multiagent_environments
    raise ValueError(
ValueError: Your environment (<ParallelPettingZooEnv instance>) does not abide to the new gymnasium-style API!
From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
In particular, the `step()` method seems to be faulty.
Learn more about the most important changes here:
https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium

In order to fix this problem, do the following:

1) Run `pip install gymnasium` on your command line.
2) Change all your import statements in your code from
   `import gym` -> `import gymnasium as gym` OR
   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`

For your custom (single agent) gym.Env classes:
3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import
     EnvCompatibility` wrapper class.
3.2) Alternatively to 3.1:
 - Change your `reset()` method to have the call signature 'def reset(self, *,
   seed=None, options=None)'
 - Return an additional info dict (empty dict should be fine) from your `reset()`
   method.
 - Return an additional `truncated` flag from your `step()` method (between `done` and
   `info`). This flag should indicate, whether the episode was terminated prematurely
   due to some time constraint or other kind of horizon setting.

For your custom RLlib `MultiAgentEnv` classes:
4.1) Either wrap your old MultiAgentEnv via the provided
     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import
     MultiAgentEnvCompatibility` wrapper class.
4.2) Alternatively to 4.1:
 - Change your `reset()` method to have the call signature
   'def reset(self, *, seed=None, options=None)'
 - Return an additional per-agent info dict (empty dict should be fine) from your
   `reset()` method.
 - Rename `dones` into `terminateds` and only set this to True, if the episode is really
   done (as opposed to has been terminated prematurely due to some horizon/time-limit
   setting).
 - Return an additional `truncateds` per-agent dictionary flag from your `step()`
   method, including the `__all__` key (100% analogous to your `dones/terminateds`
   per-agent dict).
   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This
   flag should indicate, whether the episode (for some agent or all agents) was
   terminated prematurely due to some time constraint or other kind of horizon setting.


The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).

During handling of the above exception, another exception occurred:

[36mray::PPO.__init__()[39m (pid=111147, ip=172.23.148.251, actor_id=8eb16805caaf7d7159be104d01000000, repr=PPO)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 516, in __init__
    super().__init__(
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py", line 161, in __init__
    self.setup(copy.deepcopy(self.config))
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 638, in setup
    self.workers = WorkerSet(
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py", line 181, in __init__
    raise e.args[0].args[2]
ValueError: Traceback (most recent call last):
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py", line 363, in check_multiagent_environments
    results = env.step(sampled_action)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py", line 206, in step
    obss, rews, terminateds, truncateds, infos = self.par_env.step(action_dict)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/conversions.py", line 207, in step
    self.aec_env.step(actions[agent])
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/order_enforcing.py", line 91, in step
    super().step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py", line 116, in step
    self.env.step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/clip_out_of_bounds.py", line 52, in step
    super().step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py", line 116, in step
    self.env.step(action)
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py", line 253, in step
    self._execute_world_step()
  File "/usr/local/lib/python3.10/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py", line 173, in _execute_world_step
    scenario_action.append(action[0:mdim])
TypeError: 'int' object is not subscriptable

The above exception was the direct cause of the following exception:

[36mray::PPO.__init__()[39m (pid=111147, ip=172.23.148.251, actor_id=8eb16805caaf7d7159be104d01000000, repr=PPO)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py", line 81, in check_env
    check_multiagent_environments(env)
  File "/home/kuros/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py", line 368, in check_multiagent_environments
    raise ValueError(
ValueError: Your environment (<ParallelPettingZooEnv instance>) does not abide to the new gymnasium-style API!
From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
In particular, the `step()` method seems to be faulty.
Learn more about the most important changes here:
https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium

In order to fix this problem, do the following:

1) Run `pip install gymnasium` on your command line.
2) Change all your import statements in your code from
   `import gym` -> `import gymnasium as gym` OR
   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`

For your custom (single agent) gym.Env classes:
3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import
     EnvCompatibility` wrapper class.
3.2) Alternatively to 3.1:
 - Change your `reset()` method to have the call signature 'def reset(self, *,
   seed=None, options=None)'
 - Return an additional info dict (empty dict should be fine) from your `reset()`
   method.
 - Return an additional `truncated` flag from your `step()` method (between `done` and
   `info`). This flag should indicate, whether the episode was terminated prematurely
   due to some time constraint or other kind of horizon setting.

For your custom RLlib `MultiAgentEnv` classes:
4.1) Either wrap your old MultiAgentEnv via the provided
     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import
     MultiAgentEnvCompatibility` wrapper class.
4.2) Alternatively to 4.1:
 - Change your `reset()` method to have the call signature
   'def reset(self, *, seed=None, options=None)'
 - Return an additional per-agent info dict (empty dict should be fine) from your
   `reset()` method.
 - Rename `dones` into `terminateds` and only set this to True, if the episode is really
   done (as opposed to has been terminated prematurely due to some horizon/time-limit
   setting).
 - Return an additional `truncateds` per-agent dictionary flag from your `step()`
   method, including the `__all__` key (100% analogous to your `dones/terminateds`
   per-agent dict).
   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This
   flag should indicate, whether the episode (for some agent or all agents) was
   terminated prematurely due to some time constraint or other kind of horizon setting.


The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).
