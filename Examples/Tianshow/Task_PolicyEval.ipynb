{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programas\\b_ace_env\\Lib\\site-packages\\wandb\\analytics\\sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
      "  self.hub = sentry_sdk.Hub(client)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "import pandas as pd \n",
    "\n",
    "import random\n",
    "\n",
    "from torch.distributions import Normal, Distribution\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import SubprocVectorEnv, DummyVectorEnv\n",
    "#from tianshou.env.pettingzoo_env_parallel import PettingZooParallelEnv\n",
    "#from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "#from PettingZooParallelEnv import PettingZooParallelEnv\n",
    "\n",
    "\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "\n",
    "from tianshou.utils.net.common import ActorCritic, DataParallelNet, Net\n",
    "from tianshou.utils.net.continuous import Actor, Critic\n",
    "\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, DDPGPolicy\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from DNN_B_ACE_ACTOR import DNN_B_ACE_ACTOR\n",
    "from DNN_B_ACE_CRITIC import DNN_B_ACE_CRITIC\n",
    "from Task_MHA_B_ACE import Task_MHA_B_ACE\n",
    "from Task_DNN_B_ACE import Task_DNN_B_ACE\n",
    "from Task_B_ACE_Env import B_ACE_TaskEnv\n",
    "\n",
    "from CollectorMA import CollectorMA\n",
    "from MAParalellPolicy import MAParalellPolicy\n",
    "\n",
    "\n",
    "####---------------------------#######\n",
    "#Tianshou Adjustment\n",
    "import wandb\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Tianshow_Training_GoDot.ipybn\"\n",
    "from tianshou.utils import WandbLogger\n",
    "# from tianshou.utils.logger.base import LOG_DATA_TYPE\n",
    "# def new_write(self, step_type: str, step: int, data: LOG_DATA_TYPE) -> None:\n",
    "#      data[step_type] = step\n",
    "#      wandb.log(data)   \n",
    "# WandbLogger.write = new_write \n",
    "####---------------------------#######\n",
    "\n",
    "\n",
    "model  =  \"Task_MHA_B_ACE\"#\"SISL_Task_MultiHead\" #\"CNN_ATT_SISL\" #\"MultiHead_SISL\" \n",
    "test_num  =  \"_B_ACE03\"\n",
    "policyModel  =  \"DQN\"\n",
    "name = model + test_num\n",
    "\n",
    "train_env_num = 4\n",
    "test_env_num  = 15\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn_sisl\", log_name)\n",
    "\n",
    "load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE02240721-151049_1261_BestRew.pth'\n",
    "save_policy_name = f'policy_{log_name}'\n",
    "policy_path = model + policyModel\n",
    "\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "Policy_Config = {\n",
    "    \"same_policy\" : True,\n",
    "    \"load_model\" : False,\n",
    "    \"freeze_CNN\" : False     \n",
    "                }\n",
    "\n",
    "B_ACE_Config = { \t\n",
    "                    \"EnvConfig\" : \n",
    "                    {\n",
    "                        \"task\": \"b_ace_v1\",\n",
    "                        \"env_path\": \"..\\..\\BVR_AirCombat/bin/B_ACE_v13.exe\",\n",
    "                        \"port\": 12500,\n",
    "                        \"renderize\": 1,\n",
    "                        \"debug_view\": 0,\n",
    "                        \"phy_fps\": 20,\n",
    "                        \"speed_up\": 50000,\n",
    "                        \"max_cycles\": 36000,\n",
    "                        \"experiment_mode\"  : 0,\n",
    "                        \"parallel_envs\": 1,\t\n",
    "                        \"seed\": 1,\t\n",
    "                        \"action_repeat\": 20,\t\n",
    "                        \"action_type\": \"Low_Level_Continuous\",                        \n",
    "                        \"stop_mission\" : 1,\n",
    "                        \n",
    "                        \n",
    "                        \"RewardsConfig\" : {\n",
    "                                    \"mission_factor\": 0.001,\t\t\t\t\n",
    "                                    \"missile_fire_factor\": -0.1,\t\t\n",
    "                                    \"missile_no_fire_factor\": -0.001,\n",
    "                                    \"missile_miss_factor\": -0.5,\n",
    "                                    \"detect_loss_factor\": -0.1,\n",
    "                                    \"keep_track_factor\": 0.001,\n",
    "                                    \"hit_enemy_factor\": 3.0,\n",
    "                                    \"hit_own_factor\": -5.0,\t\t\t\n",
    "                                    \"mission_accomplished_factor\": 10.0,\t\t\t\n",
    "                                }\n",
    "                    },\n",
    "\n",
    "                    \"AgentsConfig\" : \n",
    "                    {\n",
    "                        \"blue_agents\": { \n",
    "                            \"num_agents\" : 1,\n",
    "                            \"mission\"    : \"DCA\",\n",
    "                            \"beh_config\" : {\n",
    "                                            \"dShot\" : [1.04, 0.50, 1.09],\n",
    "                                            \"lCrank\": [1.06, 0.98, 0.98],\n",
    "                                            \"lBreak\": [1.05, 1.17, 0.45]\n",
    "                                        },\n",
    "                            \"base_behavior\": \"external\",                  \n",
    "                            \"init_position\": {\"x\": 0.0, \"y\": 25000.0,\"z\": 30.0},\n",
    "                            \"offset_pos\": {\t\"x\": 0.0, \"y\": 0.0, \"z\": 0.0},\n",
    "                            \"init_hdg\": 0.0,                        \n",
    "                            \"target_position\": {\"x\": 0.0,\"y\": 25000.0,\"z\": 30.0},\n",
    "                            \"rnd_offset_range\":{\"x\": 10.0,\"y\": 10000.0,\"z\": 5.0},\t\t\t\t\n",
    "                            \"rnd_shot_dist_var\": 0.025,\n",
    "                            \"rnd_crank_var\": 0.025,\n",
    "                            \"rnd_break_var\": 0.025,\n",
    "                            \"wez_models\" : \"res://assets/wez/Default_Wez_params.json\"\n",
    "                        },\t\n",
    "                        \"red_agents\":\n",
    "                        { \n",
    "                            \"num_agents\" : 1, \n",
    "                            \"base_behavior\": \"baseline1\",\n",
    "                            \"mission\"    : \"striker\",\n",
    "                            # \"beh_config\" : {\n",
    "                            #                 \"dShot\" : [1.04, 1.04, 1.04], #[1.04, 0.50, 1.09]\n",
    "                            #                 \"lCrank\": [1.06, 1.06, 1.06], #1.06, 0.98, 0.98\n",
    "                            #                 \"lBreak\": [1.05, 1.05, 1.05], #1.05, 1.17, 0.45\n",
    "                            #             },\n",
    "                             \"beh_config\" : {\n",
    "                                            \"dShot\" : [1.04, 0.50, 1.09],\n",
    "                                            \"lCrank\": [1.06, 0.98, 0.98],\n",
    "                                            \"lBreak\": [1.05, 1.17, 0.45]\n",
    "                                        },\n",
    "                            \"init_position\": {\"x\": 0.0,\"y\": 25000.0,\"z\": -30.0},\n",
    "                            \"offset_pos\": {\"x\": 0.0,\"y\": 0.0,\"z\": 0.0},\n",
    "                            \"init_hdg\" : 180.0,                        \n",
    "                            \"target_position\": {\"x\": 0.0,\"y\": 25000.0,\"z\": 30.0},\n",
    "                            \"rnd_offset_range\":{\"x\": 10.0,\"y\": 10000.0,\"z\": 5.0},\t\t\t\t\n",
    "                            \"rnd_shot_dist_var\": 0.025,\n",
    "                            \"rnd_crank_var\": 0.025,\n",
    "                            \"rnd_break_var\": 0.025,\n",
    "                            \"wez_models\" : \"res://assets/wez/Default_Wez_params.json\"\n",
    "                        }\n",
    "                    }\t\n",
    "            }\n",
    "#max_cycles = B_ACE_Config[\"max_cycles\"]\n",
    "#n_agents = 1#B_ACE_Config[\"n_pursuers\"]\n",
    "\n",
    "dqn_params =    {\n",
    "                \"discount_factor\": 0.99, \n",
    "                \"estimation_step\": 180, \n",
    "                \"target_update_freq\": 6000 * 3 ,#max_cycles * n_agents,\n",
    "                \"reward_normalization\" : False,\n",
    "                \"clip_loss_grad\" : False,\n",
    "                \"optminizer\": \"Adam\",\n",
    "                \"lr\": 0.00005, \n",
    "                \"max_tasks\" : 30\n",
    "                }\n",
    "\n",
    "PPO_params= {    \n",
    "                'action_scaling': True,\n",
    "                'discount_factor': 0.98,\n",
    "                'max_grad_norm': 0.5,\n",
    "                'eps_clip': 0.2,\n",
    "                'vf_coef': 0.5,\n",
    "                'ent_coef': 0.01,\n",
    "                'gae_lambda': 0.95,\n",
    "                'reward_normalization': False, \n",
    "                'dual_clip': None,\n",
    "                'value_clip': False,   \n",
    "                'deterministic_eval': True,\n",
    "                'advantage_normalization': False,\n",
    "                'recompute_advantage': False,\n",
    "                'action_bound_method': \"clip\",\n",
    "                'lr_scheduler': None,\n",
    "            }\n",
    "\n",
    "\n",
    "trainer_params = {\"max_epoch\": 500,\n",
    "                  \"step_per_epoch\": 18000,#5 * (150 * n_agents),\n",
    "                  \"step_per_collect\": 6000,# * (10 * n_agents),\n",
    "                  \n",
    "                  \"batch_size\" : 1024,\n",
    "                  \n",
    "                  \"update_per_step\": 1 / (100), #Off-Policy Only (run after close a Collect (run many times as necessary to meet the value))\n",
    "                  \n",
    "                  \"repeat_per_collect\": 32, #On-Policy Only\n",
    "                  \n",
    "                  \"episode_per_test\": 30,                  \n",
    "                  \"tn_eps_max\": 0.20,\n",
    "                  \"ts_eps_max\": 0.01,\n",
    "                  \"warmup_size\" : 1,\n",
    "                  \"train_envs\" : train_env_num,\n",
    "                  \"test_envs\" : test_env_num\n",
    "}\n",
    "#agent_learn = PPOPolicy(**policy_params)\n",
    "\n",
    "\n",
    "runConfig = dqn_params\n",
    "runConfig.update(Policy_Config)\n",
    "runConfig.update(B_ACE_Config)\n",
    "runConfig.update(trainer_params) \n",
    "runConfig.update(dqn_params)\n",
    "\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()       \n",
    "    agent_observation_space = env.observation_space(\"agent_0\")\n",
    "   \n",
    "    #print(env.action_space)\n",
    "    #action_shape = 50#env.action_space.shape\n",
    "    \n",
    "    print(\"ActionSPACE: \", env.action_space)\n",
    "    action_space = env.action_space\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"  \n",
    "\n",
    "    agents = []        \n",
    "    \n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "        policies_number = 1\n",
    "    else:\n",
    "        policies_number = len(env.agents)\n",
    "\n",
    "    for _ in range(policies_number):      \n",
    "        \n",
    "        #print(agent_observation_space)\n",
    "        \n",
    "        if policyModel == \"DQN\":\n",
    "\n",
    "            if model == \"Task_MHA_B_ACE\":\n",
    "                net = Task_MHA_B_ACE(\n",
    "                    #obs_shape=agent_observation_space.shape,                                                  \n",
    "                    num_tasks = dqn_params[\"max_tasks\"],\n",
    "                    num_features_per_task= 14,                    \n",
    "                    nhead = 4,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                    \n",
    "                ).to(device) \n",
    "            \n",
    "            if model == \"Task_DNN_B_ACE\":\n",
    "                net = Task_DNN_B_ACE(\n",
    "                    #obs_shape=agent_observation_space.shape,                                                  \n",
    "                    num_tasks = dqn_params[\"max_tasks\"],\n",
    "                    num_features_per_task= 14,                    \n",
    "                    nhead = 4,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                    \n",
    "                ).to(device) \n",
    "                \n",
    "            optim = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True)       \n",
    "            \n",
    "            agent_learn = DQNPolicy(\n",
    "                model=net,\n",
    "                optim=optim,\n",
    "                action_space = Discrete(dqn_params[\"max_tasks\"]),\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "                reward_normalization = dqn_params[\"reward_normalization\"],\n",
    "                clip_loss_grad = dqn_params[\"clip_loss_grad\"]\n",
    "            )                   \n",
    "        \n",
    "        elif model == \"PPO_DNN\":\n",
    "            \n",
    "            actor = DNN_B_ACE_ACTOR(\n",
    "                obs_shape=agent_observation_space.shape[0],                \n",
    "                action_shape=4,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"                \n",
    "            ).to(device)\n",
    "\n",
    "            critic = DNN_B_ACE_CRITIC(\n",
    "                obs_shape=agent_observation_space.shape[0],                \n",
    "                action_shape=4,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"                \n",
    "            ).to(device)\n",
    "            \n",
    "                                    \n",
    "            actor_critic = ActorCritic(actor, critic)\n",
    "        \n",
    "            # orthogonal initialization\n",
    "            # for m in actor_critic.modules():\n",
    "            #     if isinstance(m, torch.nn.Linear):\n",
    "            #         torch.nn.init.orthogonal_(m.weight)\n",
    "            #         torch.nn.init.zeros_(m.bias)            \n",
    "            \n",
    "            # dist = torch.distributions.Normal(torch.tensor([0.0]), torch.tensor([1.0])) \n",
    "                # define policy\n",
    "            def dist(mu, sigma) -> Distribution:\n",
    "                return Normal(mu, sigma)        \n",
    "                \n",
    "            #optim_actor  = torch.optim.Adam(netActor.parameters(),  lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "            #optim_critic = torch.optim.Adam(netCritic.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "            optim = torch.optim.Adam(actor_critic.parameters(), lr=dqn_params[\"lr\"])\n",
    "                    \n",
    "            agent_learn = PPOPolicy(\n",
    "                actor=actor,\n",
    "                critic=critic,\n",
    "                optim=optim,\n",
    "                dist_fn=dist,                \n",
    "                action_scaling  =       PPO_params['action_scaling'],\n",
    "                discount_factor =       PPO_params['discount_factor'],\n",
    "                max_grad_norm   =       PPO_params['max_grad_norm'],\n",
    "                eps_clip        =       PPO_params['eps_clip'],\n",
    "                vf_coef         =       PPO_params['vf_coef'],\n",
    "                ent_coef        =       PPO_params['ent_coef'],\n",
    "                gae_lambda      =       PPO_params['gae_lambda'],\n",
    "                reward_normalization=   PPO_params['reward_normalization'],\n",
    "                action_space    =  action_space,\n",
    "                deterministic_eval=     PPO_params['deterministic_eval'],\n",
    "                advantage_normalization=PPO_params['advantage_normalization'],\n",
    "                recompute_advantage=    PPO_params['recompute_advantage'],\n",
    "                action_bound_method=    PPO_params['action_bound_method'],\n",
    "                lr_scheduler=None\n",
    "            )\n",
    "            \n",
    "        if Policy_Config[\"load_model\"] is True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "                   \n",
    "        \n",
    "        agents.append(agent_learn)\n",
    "\n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "        agents = [agents[0] for _ in range(len(env.agents))]\n",
    "    else:\n",
    "        for _ in range(len(env.agents) - policies_number):\n",
    "            agents.append(agents[0])\n",
    "    \n",
    "    policy = MultiAgentPolicyManager(policies = agents, env=env)  \n",
    "    #policy = MAParalellPolicy(policies = agents, env=env, device=\"cuda\" if torch.cuda.is_available() else \"cpu\" )  \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    \n",
    "    B_ACE_Config[\"EnvConfig\"][\"seed\"] = random.randint(0, 1000000)\n",
    "    \n",
    "    env = B_ACE_TaskEnv( convert_action_space = True,\n",
    "                                    device = \"cpu\",\n",
    "                                    **B_ACE_Config)\n",
    "    \n",
    "    #env.action_space = env.action_space()\n",
    "    #env = PettingZooEnv(env)  \n",
    "    \n",
    "    return env  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for remote GODOT connection on port 11836\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [22], 'space': 'box'}}\n",
      "ActionSPACE:  Discrete(30)\n",
      "waiting for remote GODOT connection on port 12392\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [22], 'space': 'box'}}\n",
      "waiting for remote GODOT connection on port 11397\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [22], 'space': 'box'}}\n",
      "waiting for remote GODOT connection on port 13383\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [22], 'space': 'box'}}\n",
      "waiting for remote GODOT connection on port 12741\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [22], 'space': 'box'}}\n",
      "waiting for remote GODOT connection on port 13267\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [22], 'space': 'box'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_66768\\2958220218.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy.policies[agent].load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  7.73477341169898\n",
      "Std:   8.744787774644204\n",
      "Max:   16.127168835667927\n",
      "Min:   -10.653523812569645\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def _get_envT():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    # env_paralell = MultiUAVEnv()  \n",
    "    # env = pursuit_v4.env()\n",
    "\n",
    "    env =  TaskPursuitEnv.env(\n",
    "                max_cycles=SISL_Config[\"max_cycles\"],\n",
    "                x_size=SISL_Config[\"x_size\"],\n",
    "                y_size=SISL_Config[\"y_size\"],\n",
    "                shared_reward=SISL_Config[\"shared_reward\"],\n",
    "                n_evaders=SISL_Config[\"n_evaders\"],\n",
    "                n_pursuers=SISL_Config[\"n_pursuers\"],\n",
    "                obs_range=7,#[5,5],#SISL_Config[\"obs_range\"],\n",
    "                n_catch=SISL_Config[\"n_catch\"],\n",
    "                freeze_evaders=SISL_Config[\"freeze_evaders\"],\n",
    "                tag_reward=SISL_Config[\"tag_reward\"],\n",
    "                catch_reward=SISL_Config[\"catch_reward\"],\n",
    "                urgency_reward=SISL_Config[\"urgency_reward\"],\n",
    "                surround=SISL_Config[\"surround\"],\n",
    "                constraint_window=SISL_Config[\"constraint_window\"],\n",
    "                # att_memory = SISL_Config[\"att_memory\"],\n",
    "                #render_mode= \"human\"#True\n",
    "                render_mode= None#\"human\"#True\n",
    "            ) \n",
    "           \n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    # env = CustomParallelToAECWrapper(env_paralell)\n",
    "    return PettingZooEnv(env)\n",
    "    # return PettingZooParallelEnv(env)   \n",
    "\n",
    "\n",
    "policy, optim, agents = _get_agents()\n",
    "test_env_num = 5\n",
    " # ======== Step 1: Environment setup =========\n",
    "\n",
    "test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "#SubprocVectorEnv\n",
    "\n",
    "# seed\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "#for t_env in test_envs:\n",
    "#    t_env.seed(np.random.random())\n",
    "\n",
    "episodes =  30\n",
    "render  = False\n",
    "\n",
    "policy_name = \"policy_Task_MHA_B_ACE_B_ACE03241018-082438_425_BestRew.pth\" \n",
    "\n",
    "#policy_name = \"policy_CNN_SISL_Desk_CNN02240128-083000_2618_BestRew.pth\"\n",
    "# Load the saved checkpoint\n",
    "for agent in agents:    \n",
    "    \n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "         model_path = os.path.join(\"Task_MHA_B_ACEDQN\", policy_name)                            \n",
    "    else:\n",
    "         model_path = os.path.join(\"Task_MHA_B_ACEDQN\", policy_name) \n",
    "\n",
    "    policy.policies[agent].set_eps(0.00)\n",
    "    policy.policies[agent].load_state_dict(torch.load(model_path))\n",
    "    policy.policies[agent].eval()\n",
    "    \n",
    "test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "\n",
    "results = test_collector.collect(n_episode=episodes)#0.02)#, gym_reset_kwargs={'seed' :2})\n",
    "\n",
    "print(\"Mean: \", np.mean(results.returns))\n",
    "print(\"Std:  \" , np.std (results.returns))\n",
    "print(\"Max:  \" , np.max( results.returns))\n",
    "print(\"Min:  \" , np.min(results.returns))\n",
    "\n",
    "\n",
    "#Gets Final Stats\n",
    "methods = test_envs.get_env_attr(\"call_results\")  # This returns a list of method references\n",
    "results = [method() for method in methods]  # Call each method\n",
    "df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Sample data (shortened for illustration; use your full data in practice)\n",
    "data = results\n",
    "\n",
    "blue_team_data = []\n",
    "red_team_data = []\n",
    "general_data = []\n",
    "\n",
    "for episode in data:\n",
    "    for entry in episode:\n",
    "        blue_team_data.append(entry[0])  # First dictionary: Blue team\n",
    "        red_team_data.append(entry[1])   # Second dictionary: Red team\n",
    "        general_data.append(entry[2])    # Third dictionary: General sim data\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_blue = pd.DataFrame(blue_team_data)\n",
    "df_blue = df_blue[df_blue.end_cond.notna()]\n",
    "\n",
    "df_red = pd.DataFrame(red_team_data)\n",
    "df_red = df_red[df_red.end_cond.notna()]\n",
    "\n",
    "df_general = pd.DataFrame(general_data)\n",
    "\n",
    "# Merge DataFrames for a complete view (optional)\n",
    "df_merged = pd.concat([df_general, df_blue.add_prefix('blue_'), df_red.add_prefix('red_')], axis=1)\n",
    "df_merged = df_merged[df_merged.blue_end_cond.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   team   metric      mean  ci_lower   ci_upper\n",
      "0  Blue   killed  0.000000  0.000000   0.000000\n",
      "1   Red   killed  0.333333  0.166667   0.500000\n",
      "2  Blue  missile  1.500000  0.900000   2.033333\n",
      "3   Red  missile  0.166667  0.033333   0.266667\n",
      "4  Blue  mission  0.000000  0.000000   0.000000\n",
      "5   Red  mission  0.000000  0.000000   0.000000\n",
      "6  Blue   reward  7.734773  4.831504  11.060951\n",
      "7   Red   reward  0.000000  0.000000   0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "# Function to compute mean and 95% bootstrap confidence interval\n",
    "def compute_mean_and_ci(data, confidence_level=0.95):\n",
    "    mean_value = np.mean(data)\n",
    "    # Perform bootstrap resampling to compute confidence interval\n",
    "    res = bootstrap((data,), np.mean, confidence_level=confidence_level, n_resamples=10000, method='basic')\n",
    "    ci_lower, ci_upper = res.confidence_interval\n",
    "    return mean_value, ci_lower, ci_upper\n",
    "\n",
    "# Compute mean and confidence intervals for Blue and Red team metrics\n",
    "metrics = ['killed', 'missile', 'mission', 'reward']\n",
    "\n",
    "final_results = {'team': [], 'metric': [], 'mean': [], 'ci_lower': [], 'ci_upper': []}\n",
    "\n",
    "for metric in metrics:\n",
    "    for team, df in [('Blue', df_blue), ('Red', df_red)]:\n",
    "        mean_value, ci_lower, ci_upper = compute_mean_and_ci(df[metric])\n",
    "        final_results['team'].append(team)\n",
    "        final_results['metric'].append(metric)\n",
    "        final_results['mean'].append(mean_value)\n",
    "        final_results['ci_lower'].append(ci_lower)\n",
    "        final_results['ci_upper'].append(ci_upper)\n",
    "\n",
    "# Convert the results into a DataFrame for display\n",
    "df_final_results = pd.DataFrame(final_results)\n",
    "print(df_final_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b_ace_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
